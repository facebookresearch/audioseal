{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Benchmarking Audioseal on the SHUSH attack applied on RAVDESS Dataset\n","\n","In this notebook, we outline the steps taken to benchmark the Audioseal architecture against different attacks on a dataset of audio files.  \n","In particular, we follow these steps:\n","- Load audio files from a dataset \n","- Watermark each audio file using Audioseal\n","- Perform perturbations/attacks to the audio files\n","- Detect the watermarks on these attacked files and keep track of the confidence of Audioseal in its predictions that the files are watermarked.\n","\n","\n","For a better understanding of Audioseal and its functionalities, it is highly recommended to go through the [Getting started notebook](https://github.com/facebookresearch/audioseal/blob/main/examples/Getting_started.ipynb)."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Dataset\n","\n","We use the [RAVDESS Emotional Speech audio](https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio) dataset for this experiment.   \n","When added to a Kaggle notebook environment, all input datasets are stored in the read-only `/kaggle/input` path. If you are not using Kaggle, or have stored your files elsewhere, you can load nested audio files by modifying `PARENT_FILES_DIR` in the cell below."]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-15T06:51:03.909949Z","iopub.status.busy":"2024-03-15T06:51:03.909220Z","iopub.status.idle":"2024-03-15T06:51:04.354260Z","shell.execute_reply":"2024-03-15T06:51:04.353256Z","shell.execute_reply.started":"2024-03-15T06:51:03.909904Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of input files: 2880\n"]}],"source":["import numpy as np \n","import pandas as pd\n","import os\n","\n","all_input_files = []\n","PARENT_FILES_DIR = '/kaggle/input'\n","\n","for dirname, _, filenames in os.walk(PARENT_FILES_DIR):\n","    for filename in filenames:\n","        if \"wav\" in filename:\n","            all_input_files.append(os.path.join(dirname, filename))\n","            \n","print(f\"Number of input files: {len(all_input_files)}\")"]},{"cell_type":"markdown","metadata":{},"source":["### Installations and Imports "]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-15T06:51:05.910354Z","iopub.status.busy":"2024-03-15T06:51:05.909237Z","iopub.status.idle":"2024-03-15T06:51:20.351281Z","shell.execute_reply":"2024-03-15T06:51:20.350239Z","shell.execute_reply.started":"2024-03-15T06:51:05.910319Z"},"trusted":true},"outputs":[],"source":["import sys\n","!{sys.executable} -m pip install -q torchaudio soundfile matplotlib audioseal\n","\n","import typing as tp\n","import julius\n","import torch\n","import torchaudio\n","import urllib"]},{"cell_type":"markdown","metadata":{},"source":["### Load Audioseal models"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-15T06:51:20.354221Z","iopub.status.busy":"2024-03-15T06:51:20.353436Z","iopub.status.idle":"2024-03-15T06:51:20.378701Z","shell.execute_reply":"2024-03-15T06:51:20.377805Z","shell.execute_reply.started":"2024-03-15T06:51:20.354185Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]}],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using device: {device}\")"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-15T06:51:20.380975Z","iopub.status.busy":"2024-03-15T06:51:20.379901Z","iopub.status.idle":"2024-03-15T06:51:20.919397Z","shell.execute_reply":"2024-03-15T06:51:20.918564Z","shell.execute_reply.started":"2024-03-15T06:51:20.380948Z"},"trusted":true},"outputs":[],"source":["from audioseal import AudioSeal\n","\n","model = AudioSeal.load_generator(\"audioseal_wm_16bits\")\n","detector = AudioSeal.load_detector(\"audioseal_detector_16bits\")"]},{"cell_type":"markdown","metadata":{},"source":["### Helper functions to load audio data, watermark audio, and get prediction scores for audio"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-15T06:51:20.922026Z","iopub.status.busy":"2024-03-15T06:51:20.921593Z","iopub.status.idle":"2024-03-15T06:51:21.091167Z","shell.execute_reply":"2024-03-15T06:51:21.090157Z","shell.execute_reply.started":"2024-03-15T06:51:20.921992Z"},"trusted":true},"outputs":[],"source":["model = model.to(device)\n","detector = detector.to(device)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-15T06:51:21.092660Z","iopub.status.busy":"2024-03-15T06:51:21.092352Z","iopub.status.idle":"2024-03-15T06:51:21.105098Z","shell.execute_reply":"2024-03-15T06:51:21.104097Z","shell.execute_reply.started":"2024-03-15T06:51:21.092635Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Secret message: tensor([[1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1]], device='cuda:0',\n","       dtype=torch.int32)\n"]}],"source":["secret_message = torch.randint(0, 2, (1, 16), dtype=torch.int32)\n","secret_message = secret_message.to(device)\n","print(f\"Secret message: {secret_message}\")\n","\n","# Function to load an audio file from its file path\n","def load_audio_file(\n","    file_path: str\n",") -> tp.Optional[tp.Tuple[torch.Tensor, int]]:\n","    try:\n","        wav, sample_rate = torchaudio.load(file_path)\n","        return wav, sample_rate\n","    except Exception as e:\n","        print(f\"Error while loading audio: {e}\")\n","        return None\n","    \n","# Function to generate a watermark for the audio and embed it into a new audio tensor\n","def generate_watermark_audio(\n","    tensor: torch.Tensor,\n","    sample_rate: int\n",") -> tp.Optional[torch.Tensor]:\n","    try:\n","        global model, device, secret_message\n","        audios = tensor.unsqueeze(0).to(device)\n","        watermarked_audio = model(audios, sample_rate=sample_rate, message=secret_message.to(device), alpha=1)\n","        return watermarked_audio\n","\n","    \n","    except Exception as e:\n","        print(f\"Error while watermarking audio: {e}\")\n","        return None\n","    \n","# Function to get the confidence score that an audio tensor was watermarked by Audioseal\n","def detect_watermark_audio(\n","    tensor: torch.Tensor,\n","    sample_rate: int,\n","    message_threshold: float = 0.50\n",") -> tp.Optional[float]:\n","    try:\n","        global detector, device\n","        # In our analysis we are not concerned with the hidden/embedded message as of now\n","        result, _ = detector.detect_watermark(tensor, sample_rate=sample_rate, message_threshold=message_threshold)\n","        return float(result)\n","    except Exception as e:\n","        print(f\"Error while detecting watermark: {e}\")\n","        return None"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Audio attacks\n","\n","- In this notebook, we migrate the code from the [Audioseal repo](https://github.com/facebookresearch/audioseal/blob/main/examples/attacks.py).\n","- We also introduce the SHUSH attack and use it for benchmarking "]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-15T06:51:33.919489Z","iopub.status.busy":"2024-03-15T06:51:33.919084Z","iopub.status.idle":"2024-03-15T06:51:33.962401Z","shell.execute_reply":"2024-03-15T06:51:33.961261Z","shell.execute_reply.started":"2024-03-15T06:51:33.919460Z"},"trusted":true},"outputs":[],"source":["# Copyright (c) Meta Platforms, Inc. and affiliates.\n","# All rights reserved.\n","#\n","# This source code is licensed under the license found in the\n","# LICENSE file in the root directory of this source tree.\n","\n","import typing as tp\n","\n","import julius\n","import torch\n","\n","\n","def generate_pink_noise(length: int) -> torch.Tensor:\n","    \"\"\"\n","    Generate pink noise using Voss-McCartney algorithm with PyTorch.\n","    \"\"\"\n","    num_rows = 16\n","    array = torch.randn(num_rows, length // num_rows + 1)\n","    reshaped_array = torch.cumsum(array, dim=1)\n","    reshaped_array = reshaped_array.reshape(-1)\n","    reshaped_array = reshaped_array[:length]\n","    # Normalize\n","    pink_noise = reshaped_array / torch.max(torch.abs(reshaped_array))\n","    return pink_noise\n","\n","\n","def audio_effect_return(\n","    tensor: torch.Tensor, mask: tp.Optional[torch.Tensor]\n",") -> tp.Union[tp.Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:\n","    \"\"\"Return the mask if it was in the input otherwise only the output tensor\"\"\"\n","    if mask is None:\n","        return tensor\n","    else:\n","        return tensor, mask\n","\n","\n","class AudioEffects:\n","    @staticmethod\n","    def speed(\n","        tensor: torch.Tensor,\n","        speed_range: tuple = (0.5, 1.5),\n","        sample_rate: int = 16000,\n","        mask: tp.Optional[torch.Tensor] = None,\n","    ) -> tp.Union[tp.Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:\n","        \"\"\"\n","        Function to change the speed of a batch of audio data.\n","        The output will have a different length !\n","\n","        Parameters:\n","        audio_batch (torch.Tensor): The batch of audio data in torch tensor format.\n","        speed (float): The speed to change the audio to.\n","\n","        Returns:\n","        torch.Tensor: The batch of audio data with the speed changed.\n","        \"\"\"\n","        speed = torch.FloatTensor(1).uniform_(*speed_range)\n","        new_sr = int(sample_rate * 1 / speed)\n","        resampled_tensor = julius.resample_frac(tensor, sample_rate, new_sr)\n","        if mask is None:\n","            return resampled_tensor\n","        else:\n","            return resampled_tensor, torch.nn.functional.interpolate(\n","                mask, size=resampled_tensor.size(-1), mode=\"nearest-exact\"\n","            )\n","\n","    @staticmethod\n","    def updownresample(\n","        tensor: torch.Tensor,\n","        sample_rate: int = 16000,\n","        intermediate_freq: int = 32000,\n","        mask: tp.Optional[torch.Tensor] = None,\n","    ) -> tp.Union[tp.Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:\n","\n","        orig_shape = tensor.shape\n","        # upsample\n","        tensor = julius.resample_frac(tensor, sample_rate, intermediate_freq)\n","        # downsample\n","        tensor = julius.resample_frac(tensor, intermediate_freq, sample_rate)\n","\n","        assert tensor.shape == orig_shape\n","        return audio_effect_return(tensor=tensor, mask=mask)\n","\n","    @staticmethod\n","    def echo(\n","        tensor: torch.Tensor,\n","        volume_range: tuple = (0.1, 0.5),\n","        duration_range: tuple = (0.1, 0.5),\n","        sample_rate: int = 16000,\n","        mask: tp.Optional[torch.Tensor] = None,\n","    ) -> tp.Union[tp.Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:\n","        \"\"\"\n","        Attenuating the audio volume by a factor of 0.4, delaying it by 100ms,\n","        and then overlaying it with the original.\n","\n","        :param tensor: 3D Tensor representing the audio signal [bsz, channels, frames]\n","        :param echo_volume: volume of the echo signal\n","        :param sample_rate: Sample rate of the audio signal.\n","        :return: Audio signal with reverb.\n","        \"\"\"\n","\n","        # Create a simple impulse response\n","        # Duration of the impulse response in seconds\n","        duration = torch.FloatTensor(1).uniform_(*duration_range)\n","        volume = torch.FloatTensor(1).uniform_(*volume_range)\n","\n","        n_samples = int(sample_rate * duration)\n","        impulse_response = torch.zeros(n_samples).type(tensor.type()).to(tensor.device)\n","\n","        # Define a few reflections with decreasing amplitude\n","        impulse_response[0] = 1.0  # Direct sound\n","\n","        impulse_response[\n","            int(sample_rate * duration) - 1\n","        ] = volume  # First reflection after 100ms\n","\n","        # Add batch and channel dimensions to the impulse response\n","        impulse_response = impulse_response.unsqueeze(0).unsqueeze(0)\n","\n","        # Convolve the audio signal with the impulse response\n","        reverbed_signal = julius.fft_conv1d(tensor, impulse_response)\n","\n","        # Normalize to the original amplitude range for stability\n","        reverbed_signal = (\n","            reverbed_signal\n","            / torch.max(torch.abs(reverbed_signal))\n","            * torch.max(torch.abs(tensor))\n","        )\n","\n","        # Ensure tensor size is not changed\n","        tmp = torch.zeros_like(tensor)\n","        tmp[..., : reverbed_signal.shape[-1]] = reverbed_signal\n","        reverbed_signal = tmp\n","\n","        return audio_effect_return(tensor=reverbed_signal, mask=mask)\n","\n","    @staticmethod\n","    def random_noise(\n","        waveform: torch.Tensor,\n","        noise_std: float = 0.001,\n","        mask: tp.Optional[torch.Tensor] = None,\n","    ) -> tp.Union[tp.Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:\n","        \"\"\"Add Gaussian noise to the waveform.\"\"\"\n","        noise = torch.randn_like(waveform) * noise_std\n","        noisy_waveform = waveform + noise\n","        return audio_effect_return(tensor=noisy_waveform, mask=mask)\n","\n","    @staticmethod\n","    def pink_noise(\n","        waveform: torch.Tensor,\n","        noise_std: float = 0.01,\n","        mask: tp.Optional[torch.Tensor] = None,\n","    ) -> tp.Union[tp.Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:\n","        \"\"\"Add pink background noise to the waveform.\"\"\"\n","        noise = generate_pink_noise(waveform.shape[-1]) * noise_std\n","        noise = noise.to(waveform.device)\n","        # Assuming waveform is of shape (bsz, channels, length)\n","        noisy_waveform = waveform + noise.unsqueeze(0).unsqueeze(0).to(waveform.device)\n","        return audio_effect_return(tensor=noisy_waveform, mask=mask)\n","\n","    @staticmethod\n","    def lowpass_filter(\n","        waveform: torch.Tensor,\n","        cutoff_freq: float = 5000,\n","        sample_rate: int = 16000,\n","        mask: tp.Optional[torch.Tensor] = None,\n","    ) -> tp.Union[tp.Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:\n","\n","        return audio_effect_return(\n","            tensor=julius.lowpass_filter(waveform, cutoff=cutoff_freq / sample_rate),\n","            mask=mask,\n","        )\n","\n","    @staticmethod\n","    def highpass_filter(\n","        waveform: torch.Tensor,\n","        cutoff_freq: float = 500,\n","        sample_rate: int = 16000,\n","        mask: tp.Optional[torch.Tensor] = None,\n","    ) -> tp.Union[tp.Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:\n","\n","        return audio_effect_return(\n","            tensor=julius.highpass_filter(waveform, cutoff=cutoff_freq / sample_rate),\n","            mask=mask,\n","        )\n","\n","    @staticmethod\n","    def bandpass_filter(\n","        waveform: torch.Tensor,\n","        cutoff_freq_low: float = 300,\n","        cutoff_freq_high: float = 8000,\n","        sample_rate: int = 16000,\n","        mask: tp.Optional[torch.Tensor] = None,\n","    ) -> tp.Union[tp.Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:\n","        \"\"\"\n","        Apply a bandpass filter to the waveform by cascading\n","        a high-pass filter followed by a low-pass filter.\n","\n","        Parameters:\n","        - waveform (torch.Tensor): Input audio waveform.\n","        - low_cutoff (float): Lower cutoff frequency.\n","        - high_cutoff (float): Higher cutoff frequency.\n","        - sample_rate (int): The sample rate of the waveform.\n","\n","        Returns:\n","        - torch.Tensor: Filtered audio waveform.\n","        \"\"\"\n","\n","        return audio_effect_return(\n","            tensor=julius.bandpass_filter(\n","                waveform,\n","                cutoff_low=cutoff_freq_low / sample_rate,\n","                cutoff_high=cutoff_freq_high / sample_rate,\n","            ),\n","            mask=mask,\n","        )\n","\n","    @staticmethod\n","    def smooth(\n","        tensor: torch.Tensor,\n","        window_size_range: tuple = (2, 10),\n","        mask: tp.Optional[torch.Tensor] = None,\n","    ) -> tp.Union[tp.Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:\n","        \"\"\"\n","        Smooths the input tensor (audio signal) using a moving average filter with the given window size.\n","\n","        Parameters:\n","        - tensor (torch.Tensor): Input audio tensor. Assumes tensor shape is (batch_size, channels, time).\n","        - window_size (int): Size of the moving average window.\n","\n","        Returns:\n","        - torch.Tensor: Smoothed audio tensor.\n","        \"\"\"\n","\n","        window_size = int(torch.FloatTensor(1).uniform_(*window_size_range))\n","        # Create a uniform smoothing kernel\n","        kernel = torch.ones(1, 1, window_size).type(tensor.type()) / window_size\n","        kernel = kernel.to(tensor.device)\n","\n","        smoothed = julius.fft_conv1d(tensor, kernel)\n","        # Ensure tensor size is not changed\n","        tmp = torch.zeros_like(tensor)\n","        tmp[..., : smoothed.shape[-1]] = smoothed\n","        smoothed = tmp\n","\n","        return audio_effect_return(tensor=smoothed, mask=mask)\n","\n","    @staticmethod\n","    def boost_audio(\n","        tensor: torch.Tensor,\n","        amount: float = 20,\n","        mask: tp.Optional[torch.Tensor] = None,\n","    ) -> tp.Union[tp.Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:\n","        return audio_effect_return(tensor=tensor * (1 + amount / 100), mask=mask)\n","\n","    @staticmethod\n","    def duck_audio(\n","        tensor: torch.Tensor,\n","        amount: float = 20,\n","        mask: tp.Optional[torch.Tensor] = None,\n","    ) -> tp.Union[tp.Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:\n","        return audio_effect_return(tensor=tensor * (1 - amount / 100), mask=mask)\n","\n","    @staticmethod\n","    def identity(\n","        tensor: torch.Tensor, mask: tp.Optional[torch.Tensor] = None\n","    ) -> tp.Union[tp.Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:\n","        return audio_effect_return(tensor=tensor, mask=mask)\n","    \n","    @staticmethod\n","    def shush(\n","        tensor: torch.Tensor,\n","        fraction: float = 0.001,\n","        mask: tp.Optional[torch.Tensor] = None\n","    ) -> tp.Union[tp.Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:\n","        \"\"\"\n","        Sets a specified chronological fraction of indices of the input tensor (audio signal) to 0.\n","\n","        Parameters:\n","        - tensor (torch.Tensor): Input audio tensor. Assumes tensor shape is (batch_size, channels, time).\n","        - fraction (float): Fraction of indices to be set to 0 (from the start of the tensor) (default: 0.001, i.e, 0.1%)\n","\n","        Returns:\n","        - torch.Tensor: \"Shushed\" audio tensor.\n","        \"\"\"\n","        time = tensor.size(-1)\n","        shush_tensor = tensor.detach().clone()\n","        \n","        # Set the first `fraction*time` indices of the waveform to 0.0\n","        shush_tensor[:, :, :int(fraction*time)] = 0.0\n","                \n","        return audio_effect_return(tensor=shush_tensor, mask=mask)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Experimental setup\n","- `fraction` values: \\{0.1\\%, 1\\%, 10\\%, 30\\%\\}\n","- `nomenclature` : n, s, m, l\n","\n","In this notebook, we set the above parameters for the SHUSH attack and note the average confidence scores of Audioseal in predicting the presence of watermarks for these attacked audio files."]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-15T06:51:39.665411Z","iopub.status.busy":"2024-03-15T06:51:39.665025Z","iopub.status.idle":"2024-03-15T06:51:39.673355Z","shell.execute_reply":"2024-03-15T06:51:39.672473Z","shell.execute_reply.started":"2024-03-15T06:51:39.665382Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x791cf49c2890>"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["import random\n","random.seed(42)\n","torch.backends.cudnn.benchmark = True\n","np.random.seed(42)\n","torch.manual_seed(42)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-15T06:51:40.533674Z","iopub.status.busy":"2024-03-15T06:51:40.532744Z","iopub.status.idle":"2024-03-15T06:59:10.240026Z","shell.execute_reply":"2024-03-15T06:59:10.239069Z","shell.execute_reply.started":"2024-03-15T06:51:40.533640Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  5%|▌         | 148/2880 [01:38<09:22,  4.86it/s]  "]},{"name":"stdout","output_type":"stream","text":["Error while watermarking audio: Given groups=1, weight of size [32, 1, 7], expected input[1, 2, 67807] to have 1 channels, but got 2 channels instead\n","Skipping file /kaggle/input/ravdess-emotional-speech-audio/Actor_05/03-01-02-01-02-02-05.wav due to 'NoneType' object has no attribute 'shape'\n"]},{"name":"stderr","output_type":"stream","text":[" 12%|█▏        | 335/2880 [02:27<04:57,  8.56it/s]  "]},{"name":"stdout","output_type":"stream","text":["Error while watermarking audio: Given groups=1, weight of size [32, 1, 7], expected input[1, 2, 57663] to have 1 channels, but got 2 channels instead\n","Skipping file /kaggle/input/ravdess-emotional-speech-audio/Actor_01/03-01-02-01-01-02-01.wav due to 'NoneType' object has no attribute 'shape'\n"]},{"name":"stderr","output_type":"stream","text":[" 12%|█▏        | 339/2880 [02:27<04:13, 10.02it/s]"]},{"name":"stdout","output_type":"stream","text":["Error while watermarking audio: Given groups=1, weight of size [32, 1, 7], expected input[1, 2, 52324] to have 1 channels, but got 2 channels instead\n","Skipping file /kaggle/input/ravdess-emotional-speech-audio/Actor_01/03-01-08-01-02-02-01.wav due to 'NoneType' object has no attribute 'shape'\n"]},{"name":"stderr","output_type":"stream","text":[" 15%|█▍        | 425/2880 [02:45<03:49, 10.68it/s]"]},{"name":"stdout","output_type":"stream","text":["Error while watermarking audio: Given groups=1, weight of size [32, 1, 7], expected input[1, 2, 69942] to have 1 channels, but got 2 channels instead\n","Skipping file /kaggle/input/ravdess-emotional-speech-audio/Actor_20/03-01-06-01-01-02-20.wav due to 'NoneType' object has no attribute 'shape'\n"]},{"name":"stderr","output_type":"stream","text":[" 15%|█▍        | 431/2880 [02:45<03:42, 11.01it/s]"]},{"name":"stdout","output_type":"stream","text":["Error while watermarking audio: Given groups=1, weight of size [32, 1, 7], expected input[1, 2, 55528] to have 1 channels, but got 2 channels instead\n","Skipping file /kaggle/input/ravdess-emotional-speech-audio/Actor_20/03-01-03-01-02-01-20.wav due to 'NoneType' object has no attribute 'shape'\n"]},{"name":"stderr","output_type":"stream","text":[" 45%|████▍     | 1289/2880 [04:43<02:16, 11.62it/s]"]},{"name":"stdout","output_type":"stream","text":["Error while watermarking audio: Given groups=1, weight of size [32, 1, 7], expected input[1, 2, 67807] to have 1 channels, but got 2 channels instead\n","Skipping file /kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_05/03-01-02-01-02-02-05.wav due to 'NoneType' object has no attribute 'shape'\n"]},{"name":"stderr","output_type":"stream","text":[" 51%|█████▏    | 1476/2880 [05:02<02:07, 11.02it/s]"]},{"name":"stdout","output_type":"stream","text":["Error while watermarking audio: Given groups=1, weight of size [32, 1, 7], expected input[1, 2, 57663] to have 1 channels, but got 2 channels instead\n","Skipping file /kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_01/03-01-02-01-01-02-01.wav due to 'NoneType' object has no attribute 'shape'\n"]},{"name":"stderr","output_type":"stream","text":[" 51%|█████▏    | 1478/2880 [05:02<01:55, 12.10it/s]"]},{"name":"stdout","output_type":"stream","text":["Error while watermarking audio: Given groups=1, weight of size [32, 1, 7], expected input[1, 2, 52324] to have 1 channels, but got 2 channels instead\n","Skipping file /kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_01/03-01-08-01-02-02-01.wav due to 'NoneType' object has no attribute 'shape'\n"]},{"name":"stderr","output_type":"stream","text":[" 54%|█████▍    | 1564/2880 [05:10<01:57, 11.20it/s]"]},{"name":"stdout","output_type":"stream","text":["Error while watermarking audio: Given groups=1, weight of size [32, 1, 7], expected input[1, 2, 69942] to have 1 channels, but got 2 channels instead\n","Skipping file /kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_20/03-01-06-01-01-02-20.wav due to 'NoneType' object has no attribute 'shape'\n"]},{"name":"stderr","output_type":"stream","text":[" 55%|█████▍    | 1570/2880 [05:11<01:52, 11.61it/s]"]},{"name":"stdout","output_type":"stream","text":["Error while watermarking audio: Given groups=1, weight of size [32, 1, 7], expected input[1, 2, 55528] to have 1 channels, but got 2 channels instead\n","Skipping file /kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_20/03-01-03-01-02-01-20.wav due to 'NoneType' object has no attribute 'shape'\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2880/2880 [07:29<00:00,  6.40it/s]\n"]}],"source":["from tqdm import tqdm\n","\n","all_scores_n = []\n","all_scores_s = []\n","all_scores_m = []\n","all_scores_l = []\n","all_saved_files = []\n","\n","for input_file in tqdm(all_input_files):\n","    try:\n","        # Load audio\n","        audio, sample_rate = load_audio_file(input_file)\n","\n","        # Generate watermarked audio\n","        watermarked_audio = generate_watermark_audio(audio, sample_rate)\n","\n","        # Perform SHUSH attacks\n","        shush_attack_audio_n = AudioEffects.shush(watermarked_audio, fraction=0.001)\n","        shush_attack_audio_s = AudioEffects.shush(watermarked_audio, fraction=0.01)\n","        shush_attack_audio_m = AudioEffects.shush(watermarked_audio, fraction=0.1)\n","        shush_attack_audio_l = AudioEffects.shush(watermarked_audio, fraction=0.3)\n","\n","        # Compute scores\n","        shush_score_n = detect_watermark_audio(shush_attack_audio_n, sample_rate)\n","        shush_score_s = detect_watermark_audio(shush_attack_audio_s, sample_rate)\n","        shush_score_m = detect_watermark_audio(shush_attack_audio_m, sample_rate)\n","        shush_score_l = detect_watermark_audio(shush_attack_audio_l, sample_rate)\n","\n","        # Store scores\n","        all_scores_n.append(float(shush_score_n))\n","        all_scores_s.append(float(shush_score_s))\n","        all_scores_m.append(float(shush_score_m))\n","        all_scores_l.append(float(shush_score_l))\n","        all_saved_files.append(input_file)\n","    except Exception as e:\n","        print(f\"Skipping file {input_file} due to {e}\")\n","        pass"]},{"cell_type":"markdown","metadata":{},"source":["## Store results and calculate metrics"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-03-15T06:59:10.242250Z","iopub.status.busy":"2024-03-15T06:59:10.241976Z","iopub.status.idle":"2024-03-15T06:59:10.249995Z","shell.execute_reply":"2024-03-15T06:59:10.248957Z","shell.execute_reply.started":"2024-03-15T06:59:10.242224Z"},"trusted":true},"outputs":[],"source":["df = pd.DataFrame({\n","    \"input_file\" : all_saved_files,\n","    \"watermark_confidence_n\" : all_scores_n,\n","    \"watermark_confidence_s\" : all_scores_s,\n","    \"watermark_confidence_m\" : all_scores_m,\n","    \"watermark_confidence_l\" : all_scores_l,\n","})"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-03-15T06:59:10.251570Z","iopub.status.busy":"2024-03-15T06:59:10.251203Z","iopub.status.idle":"2024-03-15T06:59:10.278172Z","shell.execute_reply":"2024-03-15T06:59:10.277277Z","shell.execute_reply.started":"2024-03-15T06:59:10.251528Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>watermark_confidence_n</th>\n","      <th>watermark_confidence_s</th>\n","      <th>watermark_confidence_m</th>\n","      <th>watermark_confidence_l</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>2870.000000</td>\n","      <td>2870.000000</td>\n","      <td>2870.000000</td>\n","      <td>2870.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.998849</td>\n","      <td>0.990138</td>\n","      <td>0.900209</td>\n","      <td>0.699678</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.000776</td>\n","      <td>0.000738</td>\n","      <td>0.000763</td>\n","      <td>0.000516</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.976302</td>\n","      <td>0.967376</td>\n","      <td>0.876146</td>\n","      <td>0.694676</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>0.998877</td>\n","      <td>0.990022</td>\n","      <td>0.900083</td>\n","      <td>0.699631</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>0.998922</td>\n","      <td>0.990202</td>\n","      <td>0.900260</td>\n","      <td>0.699777</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>0.998963</td>\n","      <td>0.990334</td>\n","      <td>0.900352</td>\n","      <td>0.699923</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>0.999177</td>\n","      <td>0.990550</td>\n","      <td>0.900958</td>\n","      <td>0.700464</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       watermark_confidence_n  watermark_confidence_s  watermark_confidence_m  \\\n","count             2870.000000             2870.000000             2870.000000   \n","mean                 0.998849                0.990138                0.900209   \n","std                  0.000776                0.000738                0.000763   \n","min                  0.976302                0.967376                0.876146   \n","25%                  0.998877                0.990022                0.900083   \n","50%                  0.998922                0.990202                0.900260   \n","75%                  0.998963                0.990334                0.900352   \n","max                  0.999177                0.990550                0.900958   \n","\n","       watermark_confidence_l  \n","count             2870.000000  \n","mean                 0.699678  \n","std                  0.000516  \n","min                  0.694676  \n","25%                  0.699631  \n","50%                  0.699777  \n","75%                  0.699923  \n","max                  0.700464  "]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["df.describe()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## We note that Audioseal performs very well in recalling the watermarks - even in extreme conditions of masking the first 30\\% of the audio, the average confidence is $0.699678$. "]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":107620,"sourceId":256618,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
